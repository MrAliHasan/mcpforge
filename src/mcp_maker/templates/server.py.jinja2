"""
Auto-generated MCP Server by MCP-Maker
Source: {{ source_type }} ({{ source_uri }})

This server was generated automatically. You can customize it by editing
the tools and resources below, or regenerate it with `mcp-maker init`.

Run with: mcp-maker serve
Or directly: python mcp_server.py
"""

import traceback

{% if source_type == "sqlite" %}
import sqlite3
import threading
{% elif source_type == "postgres" %}
import psycopg2
import psycopg2.extras
import psycopg2.pool
{% elif source_type == "mysql" %}
import pymysql
import pymysql.cursors
{% elif source_type == "files" %}
import csv
import json
import os
{% endif %}

{% if source_type in ["airtable", "notion", "gsheets"] %}
import time
import threading

class TokenBucketRateLimiter:
    """A thread-safe token bucket rate limiter to prevent API 429s from parallel LLM calls."""
    def __init__(self, target_rps: float, burst_size: int = 1):
        self._target_rps = target_rps
        self._capacity = burst_size
        self._tokens = burst_size
        self._last_refill = time.monotonic()
        self._lock = threading.Lock()

    def acquire(self):
        with self._lock:
            while True:
                now = time.monotonic()
                elapsed = now - self._last_refill
                if elapsed > 0:
                    self._tokens = min(self._capacity, self._tokens + elapsed * self._target_rps)
                    self._last_refill = now
                
                if self._tokens >= 1.0:
                    self._tokens -= 1.0
                    return
                time.sleep(0.1)

# Default to 4 requests per second to stay safely under 5 req/s limits
_rate_limiter = TokenBucketRateLimiter(target_rps=4.0, burst_size=1)
{% endif %}

from mcp.server.fastmcp import FastMCP

# ─── Server Setup ───

mcp = FastMCP("{{ source_type }}-server")

{% if source_type == "sqlite" %}
# ─── Database Connection (Thread-Safe) ───

DB_PATH = "{{ schema.metadata.get('db_path', source_uri) }}"
_local = threading.local()


def _get_connection():
    """Get a thread-local SQLite connection (reused across calls)."""
    if not hasattr(_local, "conn") or _local.conn is None:
        conn = sqlite3.connect(DB_PATH, check_same_thread=False)
        conn.row_factory = sqlite3.Row
        conn.execute("PRAGMA journal_mode=WAL")  # Better concurrent read performance
        _local.conn = conn
    return _local.conn

{% for table in tables %}

# ═══════════════════════════════════════════════════════
# Table: {{ table.name }}
# Columns: {{ table.columns | map(attribute='name') | join(', ') }}
{% if table.row_count is not none %}
# Rows: {{ table.row_count }}
{% endif %}
# ═══════════════════════════════════════════════════════



{% if 'read' in ops %}
@mcp.tool()
def list_{{ table.name }}(limit: int = 50, offset: int = 0) -> list[dict]:
    """List rows from the '{{ table.name }}' table.

    Args:
        limit: Maximum number of rows to return (default: 50, max: 500).
        offset: Number of rows to skip for pagination.

    Returns:
        List of {{ table.name }} records as dictionaries.
    """
    try:
        limit = min(limit, 500)
        conn = _get_connection()
        cursor = conn.execute(
            'SELECT * FROM "{{ table.name }}" LIMIT ? OFFSET ?',
            (limit, offset),
        )
        return [dict(row) for row in cursor.fetchall()]
    except Exception as e:
        raise RuntimeError(f"list_{{ table.name }} failed: {e}") from e
{% endif %}


{% if table.primary_key_columns %}
{% set pk = table.primary_key_columns[0] %}

{% if 'read' in ops %}
@mcp.tool()
def get_{{ table.name }}_by_{{ pk.name }}({{ pk.name }}: {{ 'int' if pk.type.value == 'integer' else 'str' }}) -> dict | None:
    """Get a single row from '{{ table.name }}' by its {{ pk.name }}.

    Args:
        {{ pk.name }}: The {{ pk.name }} of the record to retrieve.

    Returns:
        The matching record as a dictionary, or None if not found.
    """
    try:
        conn = _get_connection()
        cursor = conn.execute(
            'SELECT * FROM "{{ table.name }}" WHERE "{{ pk.name }}" = ?',
            ({{ pk.name }},),
        )
        row = cursor.fetchone()
        return dict(row) if row else None
    except Exception as e:
        raise RuntimeError(f"get_{{ table.name }}_by_{{ pk.name }} failed: {e}") from e
{% endif %}

{% endif %}

{% if table.searchable_columns %}
{% set search_col = table.searchable_columns[0] %}

{% if 'read' in ops %}
@mcp.tool()
def search_{{ table.name }}(query: str, limit: int = 20) -> list[dict]:
    """Search the '{{ table.name }}' table by matching against text columns.

    This uses LIKE pattern matching (case-insensitive). For large tables,
    consider using the semantic_search tool instead for better performance.

    Args:
        query: Search text to match (case-insensitive).
        limit: Maximum results to return (default: 20).

    Returns:
        List of matching records.
    """
    try:
        limit = min(limit, 100)
        conn = _get_connection()
        conditions = []
        params = []
        {% for col in table.searchable_columns %}
        {% if col.type.value == 'string' %}
        conditions.append('"{{ col.name }}" LIKE ?')
        params.append(f"%{query}%")
        {% endif %}
        {% endfor %}

        if not conditions:
            return []

        where_clause = " OR ".join(conditions)
        cursor = conn.execute(
            f'SELECT * FROM "{{ table.name }}" WHERE {where_clause} LIMIT ?',
            (*params, limit),
        )
        return [dict(row) for row in cursor.fetchall()]
    except Exception as e:
        raise RuntimeError(f"search_{{ table.name }} failed: {e}") from e
{% endif %}

{% endif %}


{% if 'read' in ops %}
@mcp.tool()
def count_{{ table.name }}() -> int:
    """Get the total number of rows in the '{{ table.name }}' table.

    Returns:
        Total row count.
    """
    try:
        conn = _get_connection()
        cursor = conn.execute('SELECT COUNT(*) as cnt FROM "{{ table.name }}"')
        return cursor.fetchone()["cnt"]
    except Exception as e:
        raise RuntimeError(f"count_{{ table.name }} failed: {e}") from e
{% endif %}



{% if 'read' in ops %}
@mcp.tool()
def schema_{{ table.name }}() -> dict:
    """Get the schema (column names and types) of the '{{ table.name }}' table.

    Returns:
        Dictionary with column names as keys and types as values.
    """
    return {
        {% for col in table.columns %}
        "{{ col.name }}": "{{ col.type.value }}",
        {% endfor %}
    }
{% endif %}

{% if 'insert' in ops or 'update' in ops or 'delete' in ops %}
{% if table.primary_key_columns %}
{% set pk = table.primary_key_columns[0] %}


{% if 'insert' in ops %}
@mcp.tool()
def insert_{{ table.name }}({% for col in table.columns if not col.primary_key %}{{ col.name }}: {{ 'int' if col.type.value == 'integer' else ('float' if col.type.value == 'float' else ('bool' if col.type.value == 'boolean' else 'str')) }}{{ ' | None = None' if col.nullable else '' }}{{ ', ' if not loop.last else '' }}{% endfor %}) -> dict:
    """Insert a new row into the '{{ table.name }}' table.

    Returns:
        The inserted record with its generated {{ pk.name }}.
    """
    try:
        conn = _get_connection()
        columns = []
        values = []
        {% for col in table.columns if not col.primary_key %}
        if {{ col.name }} is not None:
            columns.append('"{{ col.name }}"')
            values.append({{ col.name }})
        {% endfor %}

        placeholders = ", ".join(["?" for _ in values])
        col_names = ", ".join(columns)
        cursor = conn.execute(
            f'INSERT INTO "{{ table.name }}" ({col_names}) VALUES ({placeholders})',
            values,
        )
        conn.commit()
        new_id = cursor.lastrowid
        return get_{{ table.name }}_by_{{ pk.name }}(new_id)
    except Exception as e:
        conn.rollback()
        raise RuntimeError(f"insert_{{ table.name }} failed: {e}") from e
{% endif %}



{% if 'update' in ops %}
@mcp.tool()
def update_{{ table.name }}_by_{{ pk.name }}({{ pk.name }}: {{ 'int' if pk.type.value == 'integer' else 'str' }}, {% for col in table.columns if not col.primary_key %}{{ col.name }}: {{ 'int' if col.type.value == 'integer' else ('float' if col.type.value == 'float' else ('bool' if col.type.value == 'boolean' else 'str')) }} | None = None{{ ', ' if not loop.last else '' }}{% endfor %}) -> dict | None:
    """Update an existing row in the '{{ table.name }}' table.

    Args:
        {{ pk.name }}: The {{ pk.name }} of the record to update.

    Returns:
        The updated record, or None if not found.
    """
    try:
        conn = _get_connection()
        updates = []
        values = []
        {% for col in table.columns if not col.primary_key %}
        if {{ col.name }} is not None:
            updates.append('"{{ col.name }}" = ?')
            values.append({{ col.name }})
        {% endfor %}

        if not updates:
            return get_{{ table.name }}_by_{{ pk.name }}({{ pk.name }})

        set_clause = ", ".join(updates)
        values.append({{ pk.name }})
        conn.execute(
            f'UPDATE "{{ table.name }}" SET {set_clause} WHERE "{{ pk.name }}" = ?',
            values,
        )
        conn.commit()
        return get_{{ table.name }}_by_{{ pk.name }}({{ pk.name }})
    except Exception as e:
        conn.rollback()
        raise RuntimeError(f"update_{{ table.name }}_by_{{ pk.name }} failed: {e}") from e
{% endif %}



{% if 'delete' in ops %}
@mcp.tool()
def delete_{{ table.name }}_by_{{ pk.name }}({{ pk.name }}: {{ 'int' if pk.type.value == 'integer' else 'str' }}) -> dict:
    """Delete a row from the '{{ table.name }}' table.

    Args:
        {{ pk.name }}: The {{ pk.name }} of the record to delete.

    Returns:
        A confirmation with the number of rows deleted.
    """
    try:
        conn = _get_connection()
        cursor = conn.execute(
            'DELETE FROM "{{ table.name }}" WHERE "{{ pk.name }}" = ?',
            ({{ pk.name }},),
        )
        conn.commit()
        deleted = cursor.rowcount
        return {"deleted": deleted, "{{ pk.name }}": {{ pk.name }}}
    except Exception as e:
        conn.rollback()
        raise RuntimeError(f"delete_{{ table.name }}_by_{{ pk.name }} failed: {e}") from e
{% endif %}

{% endif %}
{% endif %}
{% endfor %}

{% elif source_type == "postgres" %}
# ─── Database Connection Pool ───

DSN = "{{ source_uri }}"
_pg_pool = psycopg2.pool.ThreadedConnectionPool(minconn=1, maxconn=10, dsn=DSN)


def _get_connection():
    """Get a connection from the pool."""
    return _pg_pool.getconn()


def _put_connection(conn):
    """Return a connection to the pool."""
    _pg_pool.putconn(conn)

{% for table in tables %}

# ═══════════════════════════════════════════════════════
# Table: {{ table.name }}
# Columns: {{ table.columns | map(attribute='name') | join(', ') }}
{% if table.row_count is not none %}
# Rows: {{ table.row_count }}
{% endif %}
# ═══════════════════════════════════════════════════════



{% if 'read' in ops %}
@mcp.tool()
def list_{{ table.name }}(limit: int = 50, offset: int = 0) -> list[dict]:
    """List rows from the '{{ table.name }}' table.

    Args:
        limit: Maximum number of rows to return (default: 50, max: 500).
        offset: Number of rows to skip for pagination.
    """
    conn = _get_connection()
    try:
        limit = min(limit, 500)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        cursor.execute(
            'SELECT * FROM "{{ table.name }}" LIMIT %s OFFSET %s',
            (limit, offset),
        )
        rows = [dict(row) for row in cursor.fetchall()]
        cursor.close()
        return rows
    except Exception as e:
        raise RuntimeError(f"list_{{ table.name }} failed: {e}") from e
    finally:
        _put_connection(conn)
{% endif %}


{% if table.primary_key_columns %}
{% set pk = table.primary_key_columns[0] %}

{% if 'read' in ops %}
@mcp.tool()
def get_{{ table.name }}_by_{{ pk.name }}({{ pk.name }}: {{ 'int' if pk.type.value == 'integer' else 'str' }}) -> dict | None:
    """Get a single row from '{{ table.name }}' by its {{ pk.name }}."""
    conn = _get_connection()
    try:
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        cursor.execute(
            'SELECT * FROM "{{ table.name }}" WHERE "{{ pk.name }}" = %s',
            ({{ pk.name }},),
        )
        row = cursor.fetchone()
        cursor.close()
        return dict(row) if row else None
    except Exception as e:
        raise RuntimeError(f"get_{{ table.name }}_by_{{ pk.name }} failed: {e}") from e
    finally:
        _put_connection(conn)
{% endif %}

{% endif %}

{% if table.searchable_columns %}

{% if 'read' in ops %}
@mcp.tool()
def search_{{ table.name }}(query: str, limit: int = 20) -> list[dict]:
    """Search the '{{ table.name }}' table by matching against text columns.

    This uses ILIKE pattern matching. For large tables with millions of rows,
    consider using the semantic_search tool instead for better performance.
    """
    conn = _get_connection()
    try:
        limit = min(limit, 100)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        conditions = []
        params = []
        {% for col in table.searchable_columns %}
        {% if col.type.value == 'string' %}
        conditions.append('"{{ col.name }}" ILIKE %s')
        params.append(f"%{query}%")
        {% endif %}
        {% endfor %}

        if not conditions:
            cursor.close()
            return []

        where_clause = " OR ".join(conditions)
        params.append(limit)
        cursor.execute(
            f'SELECT * FROM "{{ table.name }}" WHERE {where_clause} LIMIT %s',
            params,
        )
        rows = [dict(row) for row in cursor.fetchall()]
        cursor.close()
        return rows
    except Exception as e:
        raise RuntimeError(f"search_{{ table.name }} failed: {e}") from e
    finally:
        _put_connection(conn)
{% endif %}

{% endif %}


{% if 'read' in ops %}
@mcp.tool()
def count_{{ table.name }}() -> int:
    """Get the total number of rows in the '{{ table.name }}' table."""
    conn = _get_connection()
    try:
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        cursor.execute('SELECT COUNT(*) as cnt FROM "{{ table.name }}"')
        count = cursor.fetchone()["cnt"]
        cursor.close()
        return count
    except Exception as e:
        raise RuntimeError(f"count_{{ table.name }} failed: {e}") from e
    finally:
        _put_connection(conn)
{% endif %}



{% if 'read' in ops %}
@mcp.tool()
def schema_{{ table.name }}() -> dict:
    """Get the schema of the '{{ table.name }}' table."""
    return {
        {% for col in table.columns %}
        "{{ col.name }}": "{{ col.type.value }}",
        {% endfor %}
    }
{% endif %}

{% if ('insert' in ops or 'update' in ops or 'delete' in ops) and table.primary_key_columns %}
{% set pk = table.primary_key_columns[0] %}


{% if 'insert' in ops %}
@mcp.tool()
def insert_{{ table.name }}({% for col in table.columns if not col.primary_key %}{{ col.name }}: {{ 'int' if col.type.value == 'integer' else ('float' if col.type.value == 'float' else ('bool' if col.type.value == 'boolean' else 'str')) }}{{ ' | None = None' if col.nullable else '' }}{{ ', ' if not loop.last else '' }}{% endfor %}) -> dict:
    """Insert a new row into the '{{ table.name }}' table."""
    conn = _get_connection()
    try:
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        columns = []
        values = []
        {% for col in table.columns if not col.primary_key %}
        if {{ col.name }} is not None:
            columns.append('"{{ col.name }}"')
            values.append({{ col.name }})
        {% endfor %}

        placeholders = ", ".join(["%s" for _ in values])
        col_names = ", ".join(columns)
        cursor.execute(
            f'INSERT INTO "{{ table.name }}" ({col_names}) VALUES ({placeholders}) RETURNING *',
            values,
        )
        row = cursor.fetchone()
        conn.commit()
        cursor.close()
        return dict(row) if row else {}
    except Exception as e:
        conn.rollback()
        raise RuntimeError(f"insert_{{ table.name }} failed: {e}") from e
    finally:
        _put_connection(conn)
{% endif %}



{% if 'update' in ops %}
@mcp.tool()
def update_{{ table.name }}_by_{{ pk.name }}({{ pk.name }}: {{ 'int' if pk.type.value == 'integer' else 'str' }}, {% for col in table.columns if not col.primary_key %}{{ col.name }}: {{ 'int' if col.type.value == 'integer' else ('float' if col.type.value == 'float' else ('bool' if col.type.value == 'boolean' else 'str')) }} | None = None{{ ', ' if not loop.last else '' }}{% endfor %}) -> dict | None:
    """Update an existing row in the '{{ table.name }}' table."""
    conn = _get_connection()
    try:
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        updates = []
        values = []
        {% for col in table.columns if not col.primary_key %}
        if {{ col.name }} is not None:
            updates.append('"{{ col.name }}" = %s')
            values.append({{ col.name }})
        {% endfor %}

        if not updates:
            cursor.close()
            return get_{{ table.name }}_by_{{ pk.name }}({{ pk.name }})

        set_clause = ", ".join(updates)
        values.append({{ pk.name }})
        cursor.execute(
            f'UPDATE "{{ table.name }}" SET {set_clause} WHERE "{{ pk.name }}" = %s RETURNING *',
            values,
        )
        row = cursor.fetchone()
        conn.commit()
        cursor.close()
        return dict(row) if row else None
    except Exception as e:
        conn.rollback()
        raise RuntimeError(f"update_{{ table.name }}_by_{{ pk.name }} failed: {e}") from e
    finally:
        _put_connection(conn)
{% endif %}



{% if 'delete' in ops %}
@mcp.tool()
def delete_{{ table.name }}_by_{{ pk.name }}({{ pk.name }}: {{ 'int' if pk.type.value == 'integer' else 'str' }}) -> dict:
    """Delete a row from the '{{ table.name }}' table."""
    conn = _get_connection()
    try:
        cursor = conn.cursor()
        cursor.execute(
            'DELETE FROM "{{ table.name }}" WHERE "{{ pk.name }}" = %s',
            ({{ pk.name }},),
        )
        deleted = cursor.rowcount
        conn.commit()
        cursor.close()
        return {"deleted": deleted, "{{ pk.name }}": {{ pk.name }}}
    except Exception as e:
        conn.rollback()
        raise RuntimeError(f"delete_{{ table.name }}_by_{{ pk.name }} failed: {e}") from e
    finally:
        _put_connection(conn)
{% endif %}

{% endif %}
{% endfor %}

{% elif source_type == "mysql" %}
# ─── Database Connection Pool ───

import queue as _queue

MYSQL_CONFIG = {
    "host": "{{ schema.metadata.get('host', 'localhost') }}",
    "port": {{ schema.metadata.get('port', 3306) }},
    "user": "{{ schema.metadata.get('user', 'root') }}",
    "password": "{{ schema.metadata.get('password', '') }}",
    "database": "{{ schema.metadata.get('database', '') }}",
}

_mysql_pool = _queue.Queue(maxsize=10)


def _get_connection():
    """Get a MySQL connection from the pool, or create a new one."""
    try:
        conn = _mysql_pool.get_nowait()
        conn.ping(reconnect=True)
        return conn
    except _queue.Empty:
        return pymysql.connect(
            **MYSQL_CONFIG,
            cursorclass=pymysql.cursors.DictCursor,
        )


def _put_connection(conn):
    """Return a MySQL connection to the pool."""
    try:
        _mysql_pool.put_nowait(conn)
    except _queue.Full:
        conn.close()

{% for table in tables %}

# ═══════════════════════════════════════════════════════
# Table: {{ table.name }}
# Columns: {{ table.columns | map(attribute='name') | join(', ') }}
{% if table.row_count is not none %}
# Rows: {{ table.row_count }}
{% endif %}
# ═══════════════════════════════════════════════════════



{% if 'read' in ops %}
@mcp.tool()
def list_{{ table.name }}(limit: int = 50, offset: int = 0) -> list[dict]:
    """List rows from the '{{ table.name }}' table."""
    conn = _get_connection()
    try:
        limit = min(limit, 500)
        cursor = conn.cursor()
        cursor.execute(
            "SELECT * FROM `{{ table.name }}` LIMIT %s OFFSET %s",
            (limit, offset),
        )
        rows = [dict(row) for row in cursor.fetchall()]
        cursor.close()
        return rows
    except Exception as e:
        raise RuntimeError(f"list_{{ table.name }} failed: {e}") from e
    finally:
        _put_connection(conn)
{% endif %}


{% if table.primary_key_columns %}
{% set pk = table.primary_key_columns[0] %}

{% if 'read' in ops %}
@mcp.tool()
def get_{{ table.name }}_by_{{ pk.name }}({{ pk.name }}: {{ 'int' if pk.type.value == 'integer' else 'str' }}) -> dict | None:
    """Get a single row from '{{ table.name }}' by its {{ pk.name }}."""
    conn = _get_connection()
    try:
        cursor = conn.cursor()
        cursor.execute(
            "SELECT * FROM `{{ table.name }}` WHERE `{{ pk.name }}` = %s",
            ({{ pk.name }},),
        )
        row = cursor.fetchone()
        cursor.close()
        return dict(row) if row else None
    except Exception as e:
        raise RuntimeError(f"get_{{ table.name }}_by_{{ pk.name }} failed: {e}") from e
    finally:
        _put_connection(conn)
{% endif %}

{% endif %}

{% if table.searchable_columns %}

{% if 'read' in ops %}
@mcp.tool()
def search_{{ table.name }}(query: str, limit: int = 20) -> list[dict]:
    """Search the '{{ table.name }}' table by matching against text columns.

    This uses LIKE pattern matching. For large tables with millions of rows,
    consider using the semantic_search tool instead for better performance.
    """
    conn = _get_connection()
    try:
        limit = min(limit, 100)
        cursor = conn.cursor()
        conditions = []
        params = []
        {% for col in table.searchable_columns %}
        {% if col.type.value == 'string' %}
        conditions.append("`{{ col.name }}` LIKE %s")
        params.append(f"%{query}%")
        {% endif %}
        {% endfor %}

        if not conditions:
            cursor.close()
            return []

        where_clause = " OR ".join(conditions)
        params.append(limit)
        cursor.execute(
            f"SELECT * FROM `{{ table.name }}` WHERE {where_clause} LIMIT %s",
            params,
        )
        rows = [dict(row) for row in cursor.fetchall()]
        cursor.close()
        return rows
    except Exception as e:
        raise RuntimeError(f"search_{{ table.name }} failed: {e}") from e
    finally:
        _put_connection(conn)
{% endif %}

{% endif %}


{% if 'read' in ops %}
@mcp.tool()
def count_{{ table.name }}() -> int:
    """Get the total number of rows in the '{{ table.name }}' table."""
    conn = _get_connection()
    try:
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) as cnt FROM `{{ table.name }}`")
        count = cursor.fetchone()["cnt"]
        cursor.close()
        return count
    except Exception as e:
        raise RuntimeError(f"count_{{ table.name }} failed: {e}") from e
    finally:
        _put_connection(conn)
{% endif %}



{% if 'read' in ops %}
@mcp.tool()
def schema_{{ table.name }}() -> dict:
    """Get the schema of the '{{ table.name }}' table."""
    return {
        {% for col in table.columns %}
        "{{ col.name }}": "{{ col.type.value }}",
        {% endfor %}
    }
{% endif %}

{% if ('insert' in ops or 'update' in ops or 'delete' in ops) and table.primary_key_columns %}
{% set pk = table.primary_key_columns[0] %}


{% if 'insert' in ops %}
@mcp.tool()
def insert_{{ table.name }}({% for col in table.columns if not col.primary_key %}{{ col.name }}: {{ 'int' if col.type.value == 'integer' else ('float' if col.type.value == 'float' else ('bool' if col.type.value == 'boolean' else 'str')) }}{{ ' | None = None' if col.nullable else '' }}{{ ', ' if not loop.last else '' }}{% endfor %}) -> dict:
    """Insert a new row into the '{{ table.name }}' table."""
    conn = _get_connection()
    try:
        cursor = conn.cursor()
        columns = []
        values = []
        {% for col in table.columns if not col.primary_key %}
        if {{ col.name }} is not None:
            columns.append("`{{ col.name }}`")
            values.append({{ col.name }})
        {% endfor %}

        placeholders = ", ".join(["%s" for _ in values])
        col_names = ", ".join(columns)
        cursor.execute(
            f"INSERT INTO `{{ table.name }}` ({col_names}) VALUES ({placeholders})",
            values,
        )
        conn.commit()
        new_id = cursor.lastrowid
        cursor.close()
        return get_{{ table.name }}_by_{{ pk.name }}(new_id)
    except Exception as e:
        conn.rollback()
        raise RuntimeError(f"insert_{{ table.name }} failed: {e}") from e
    finally:
        _put_connection(conn)
{% endif %}



{% if 'delete' in ops %}
@mcp.tool()
def delete_{{ table.name }}_by_{{ pk.name }}({{ pk.name }}: {{ 'int' if pk.type.value == 'integer' else 'str' }}) -> dict:
    """Delete a row from the '{{ table.name }}' table."""
    conn = _get_connection()
    try:
        cursor = conn.cursor()
        cursor.execute(
            "DELETE FROM `{{ table.name }}` WHERE `{{ pk.name }}` = %s",
            ({{ pk.name }},),
        )
        deleted = cursor.rowcount
        conn.commit()
        cursor.close()
        return {"deleted": deleted, "{{ pk.name }}": {{ pk.name }}}
    except Exception as e:
        conn.rollback()
        raise RuntimeError(f"delete_{{ table.name }}_by_{{ pk.name }} failed: {e}") from e
    finally:
        _put_connection(conn)
{% endif %}

{% endif %}
{% endfor %}

{% elif source_type == "airtable" %}
# ─── Airtable Connection ───

import os
from pyairtable import Api

AIRTABLE_API_KEY = os.environ.get("AIRTABLE_API_KEY", os.environ.get("AIRTABLE_TOKEN", ""))
BASE_ID = "{{ schema.metadata.get('base_id', '') }}"

if not AIRTABLE_API_KEY:
    raise RuntimeError(
        "AIRTABLE_API_KEY or AIRTABLE_TOKEN environment variable is required. "
        "Get a token from: https://airtable.com/create/tokens"
    )

api = Api(AIRTABLE_API_KEY)
base = api.base(BASE_ID)

# Table name mapping (safe name -> original Airtable name)
TABLE_NAMES = {
    {% for table in tables %}
    "{{ table.name }}": "{{ table.description }}",
    {% endfor %}
}

# View name mapping per table
TABLE_VIEWS = {
    {% for table in tables %}
    {% set views = schema.metadata.get('table_views_map', {}).get(table.name, []) %}
    "{{ table.name }}": {
        {% for view in views %}
        "{{ view.name }}": "{{ view.original }}",
        {% endfor %}
    },
    {% endfor %}
}

{% for table in tables %}

# ═══════════════════════════════════════════════════════
# Table: {{ table.description }} (as {{ table.name }})
# Fields: {{ table.columns | map(attribute='name') | join(', ') }}
{% if table.row_count is not none %}
# Rows: {{ table.row_count }}
{% endif %}
{% set views = schema.metadata.get('table_views_map', {}).get(table.name, []) %}
{% if views %}
# Views: {{ views | map(attribute='original') | join(', ') }}
{% endif %}
# ═══════════════════════════════════════════════════════

# Field name mapping for this table
_{{ table.name }}_fields = {
    {% for col in table.columns %}
    "{{ col.name }}": "{{ col.description }}",
    {% endfor %}
}
_{{ table.name }}_fields_rev = {v: k for k, v in _{{ table.name }}_fields.items()}


def _normalize_{{ table.name }}(record: dict) -> dict:
    """Convert an Airtable record to a flat dict with safe field names."""
    fields = record.get("fields", {})
    result = {"record_id": record.get("id", "")}
    for safe_name, original_name in _{{ table.name }}_fields.items():
        result[safe_name] = fields.get(original_name)
    return result


def _to_airtable_{{ table.name }}(data: dict) -> dict:
    """Convert a dict with safe field names back to Airtable field names."""
    fields = {}
    for safe_name, value in data.items():
        if safe_name == "record_id":
            continue
        if value is not None and safe_name in _{{ table.name }}_fields:
            fields[_{{ table.name }}_fields[safe_name]] = value
    return fields



{% if 'read' in ops %}
@mcp.tool()
def list_{{ table.name }}(
    limit: int = 50,
    offset: int = 0,
    sort_field: str | None = None,
    sort_direction: str = "asc",
) -> list[dict]:
    """List records from the '{{ table.description }}' table in Airtable.

    Args:
        limit: Maximum records to return (default: 50, max: 500).
        offset: Number of records to skip for pagination.
        sort_field: Field name to sort by (use safe names like 'name', 'email').
        sort_direction: Sort direction — 'asc' or 'desc'.
    """
    limit = min(limit, 500)
    tbl = base.table(TABLE_NAMES["{{ table.name }}"])

    kwargs = {"max_records": offset + limit}
    if sort_field and sort_field in _{{ table.name }}_fields:
        original_field = _{{ table.name }}_fields[sort_field]
        kwargs["sort"] = [original_field] if sort_direction == "asc" else ["-" + original_field]

    records = tbl.all(**kwargs)
    normalized = [_normalize_{{ table.name }}(r) for r in records]
    return normalized[offset:offset + limit]
{% endif %}



{% if 'read' in ops %}
@mcp.tool()
def get_{{ table.name }}_by_record_id(record_id: str) -> dict | None:
    """Get a single record from '{{ table.description }}' by its Airtable record ID.

    Args:
        record_id: The Airtable record ID (e.g. 'recXXXXXXXX').
    """
    tbl = base.table(TABLE_NAMES["{{ table.name }}"])
    try:
        # Throttle API requests to prevent 429 Too Many Requests
        _rate_limiter.acquire()
        record = tbl.get(record_id)
        return _normalize_{{ table.name }}(record)
    except Exception:
        return None
{% endif %}



{% if 'read' in ops %}
@mcp.tool()
def search_{{ table.name }}(query: str, limit: int = 20) -> list[dict]:
    """Search '{{ table.description }}' by matching against text fields.

    Args:
        query: Search text (case-insensitive, matches any text field).
        limit: Maximum results to return.
    """
    all_records = list_{{ table.name }}(limit=10000)
    query_lower = query.lower()
    results = []
    for record in all_records:
        for value in record.values():
            if value and query_lower in str(value).lower():
                results.append(record)
                break
        if len(results) >= limit:
            break
    return results
{% endif %}


@mcp.tool()
def filter_{{ table.name }}(formula: str, limit: int = 100) -> list[dict]:
    """Filter records from '{{ table.description }}' using an Airtable formula.

    This is THE most powerful query tool — it uses Airtable's native formula
    language for precise filtering. Use ORIGINAL Airtable field names in formulas.

    Args:
        formula: An Airtable formula string. Examples:
            - '{Status} = "Active"'
            - 'AND({Age} > 25, {City} = "NYC")'
            - 'FIND("python", LOWER({Skills}))'
            - '{Created} > "2024-01-01"'
            - 'NOT({Email} = BLANK())'
            - 'OR({Priority} = "High", {Priority} = "Critical")'
        limit: Maximum results (default: 100, max: 1000).

    Available fields (safe_name -> Airtable name):
        {% for col in table.columns %}
        {{ col.name }} -> {{ col.description }}
        {% endfor %}
    """
    limit = min(limit, 1000)
    tbl = base.table(TABLE_NAMES["{{ table.name }}"])
    records = tbl.all(formula=formula, max_records=limit)
    return [_normalize_{{ table.name }}(r) for r in records]



{% if 'read' in ops %}
@mcp.tool()
def count_{{ table.name }}() -> int:
    """Get the total number of records in '{{ table.description }}'."""
    tbl = base.table(TABLE_NAMES["{{ table.name }}"])
    records = tbl.all(fields=[])
    return len(records)
{% endif %}



{% if 'read' in ops %}
@mcp.tool()
def schema_{{ table.name }}() -> dict:
    """Get the schema of the '{{ table.description }}' table.

    Returns field names, types, and available options for select fields.
    """
    {% set field_opts = schema.metadata.get('field_options_map', {}).get(table.name, {}) %}
    return {
        {% for col in table.columns %}
        "{{ col.name }}": {
            "type": "{{ col.type.value }}",
            "airtable_name": "{{ col.description }}",
            {% if col.name in field_opts %}
            "options": {{ field_opts[col.name] }},
            {% endif %}
        },
        {% endfor %}
    }
{% endif %}


{% set views = schema.metadata.get('table_views_map', {}).get(table.name, []) %}
{% if views %}

{% if 'read' in ops %}
@mcp.tool()
def list_{{ table.name }}_views() -> list[dict]:
    """List all available views for the '{{ table.description }}' table.

    Views in Airtable are pre-configured filters and sorts.
    Use list_{{ table.name }}_by_view() to fetch records through a specific view.
    """
    return [
        {% for view in views %}
        {"name": "{{ view.name }}", "airtable_name": "{{ view.original }}", "type": "{{ view.type }}"},
        {% endfor %}
    ]
{% endif %}



{% if 'read' in ops %}
@mcp.tool()
def list_{{ table.name }}_by_view(view_name: str, limit: int = 100) -> list[dict]:
    """List records from '{{ table.description }}' through a specific Airtable view.

    Views apply pre-configured filters, sorts, and field visibility.
    Call list_{{ table.name }}_views() first to see available views.

    Args:
        view_name: The view name (use safe names from list_views).
            Available: {{ views | map(attribute='name') | join(', ') }}
        limit: Maximum records to return.
    """
    limit = min(limit, 1000)
    views_map = TABLE_VIEWS["{{ table.name }}"]
    original_view = views_map.get(view_name, view_name)
    tbl = base.table(TABLE_NAMES["{{ table.name }}"])
    records = tbl.all(view=original_view, max_records=limit)
    return [_normalize_{{ table.name }}(r) for r in records]
{% endif %}

{% endif %}

{% if 'insert' in ops or 'update' in ops or 'delete' in ops %}

@mcp.tool()
def create_{{ table.name }}({% for col in table.columns %}{{ col.name }}: str | None = None{{ ', ' if not loop.last else '' }}{% endfor %}) -> dict:
    """Create a new record in the '{{ table.description }}' table.

    Pass field values using safe names. All fields are optional.

    Args:
        {% for col in table.columns %}
        {{ col.name }}: Value for '{{ col.description }}'.
        {% endfor %}

    Returns:
        The newly created record with its Airtable record_id.
    """
    data = {}
    {% for col in table.columns %}
    if {{ col.name }} is not None:
        data["{{ col.name }}"] = {{ col.name }}
    {% endfor %}
    fields = _to_airtable_{{ table.name }}(data)
    tbl = base.table(TABLE_NAMES["{{ table.name }}"])
    record = tbl.create(fields)
    return _normalize_{{ table.name }}(record)



{% if 'update' in ops %}
@mcp.tool()
def update_{{ table.name }}(record_id: str, {% for col in table.columns %}{{ col.name }}: str | None = None{{ ', ' if not loop.last else '' }}{% endfor %}) -> dict | None:
    """Update an existing record in '{{ table.description }}'.

    Only provided (non-None) fields will be updated.

    Args:
        record_id: The Airtable record ID (e.g. 'recXXXXXXXX').
        {% for col in table.columns %}
        {{ col.name }}: New value for '{{ col.description }}'.
        {% endfor %}

    Returns:
        The updated record, or None if not found.
    """
    data = {}
    {% for col in table.columns %}
    if {{ col.name }} is not None:
        data["{{ col.name }}"] = {{ col.name }}
    {% endfor %}
    if not data:
        return get_{{ table.name }}_by_record_id(record_id)
    fields = _to_airtable_{{ table.name }}(data)
    tbl = base.table(TABLE_NAMES["{{ table.name }}"])
    try:
        # Throttle API requests to prevent 429 Too Many Requests
        _rate_limiter.acquire()
        record = tbl.update(record_id, fields)
        return _normalize_{{ table.name }}(record)
    except Exception:
        return None
{% endif %}



{% if 'delete' in ops %}
@mcp.tool()
def delete_{{ table.name }}(record_id: str) -> dict:
    """Delete a record from '{{ table.description }}'.

    Args:
        record_id: The Airtable record ID to delete.

    Returns:
        Confirmation with the deleted record ID.
    """
    tbl = base.table(TABLE_NAMES["{{ table.name }}"])
    try:
        # Throttle API requests to prevent 429 Too Many Requests
        _rate_limiter.acquire()
        tbl.delete(record_id)
        return {"deleted": True, "record_id": record_id}
    except Exception as e:
        return {"deleted": False, "record_id": record_id, "error": str(e)}
{% endif %}

{% endif %}
{% endfor %}

{% elif source_type == "gsheet" %}
# ─── Google Sheets Connection ───

import os
import gspread

_CREDS_FILE = os.environ.get("GOOGLE_SERVICE_ACCOUNT_FILE", "")
_CREDS_JSON = os.environ.get("GOOGLE_CREDENTIALS_JSON", "")

if _CREDS_FILE and os.path.isfile(_CREDS_FILE):
    gc = gspread.service_account(filename=_CREDS_FILE)
elif _CREDS_JSON:
    import json as _json
    import tempfile as _tempfile
    _creds = _json.loads(_CREDS_JSON)
    _tf = _tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False)
    _json.dump(_creds, _tf)
    _tf.close()
    gc = gspread.service_account(filename=_tf.name)
else:
    gc = gspread.service_account()

SPREADSHEET_ID = "{{ schema.metadata.get('spreadsheet_id', '') }}"
spreadsheet = gc.open_by_key(SPREADSHEET_ID)

# Sheet name mapping (safe name -> original sheet tab name)
SHEET_NAMES = {
    {% for table in tables %}
    "{{ table.name }}": "{{ table.description }}",
    {% endfor %}
}

{% for table in tables %}

# ═══════════════════════════════════════════════════════
# Sheet: {{ table.description }} (as {{ table.name }})
# Columns: {{ table.columns | map(attribute='name') | join(', ') }}
{% if table.row_count is not none %}
# Rows: {{ table.row_count }}
{% endif %}
# ═══════════════════════════════════════════════════════

_{{ table.name }}_fields = {
    {% for col in table.columns %}
    "{{ col.name }}": "{{ col.description }}",
    {% endfor %}
}
_{{ table.name }}_fields_rev = {v: k for k, v in _{{ table.name }}_fields.items()}


def _normalize_{{ table.name }}(record: dict, row_index: int = 0) -> dict:
    """Convert a Google Sheets row dict to use safe field names."""
    result = {"row_number": row_index + 2}  # +2 for 1-indexed + header row
    for safe_name, original_name in _{{ table.name }}_fields.items():
        result[safe_name] = record.get(original_name)
    return result



{% if 'read' in ops %}
@mcp.tool()
def list_{{ table.name }}(limit: int = 50, offset: int = 0) -> list[dict]:
    """List rows from the '{{ table.description }}' Google Sheet.

    Args:
        limit: Maximum rows (default: 50, max: 500).
        offset: Number of rows to skip.
    """
    limit = min(limit, 500)
    ws = spreadsheet.worksheet(SHEET_NAMES["{{ table.name }}"])
    records = ws.get_all_records()
    normalized = [_normalize_{{ table.name }}(r, i) for i, r in enumerate(records)]
    return normalized[offset:offset + limit]
{% endif %}



{% if 'read' in ops %}
@mcp.tool()
def search_{{ table.name }}(query: str, limit: int = 20) -> list[dict]:
    """Search the '{{ table.description }}' sheet for matching rows.

    Args:
        query: Search text (case-insensitive, matches any field).
        limit: Maximum results.
    """
    all_rows = list_{{ table.name }}(limit=10000)
    query_lower = query.lower()
    results = []
    for row in all_rows:
        for value in row.values():
            if value and query_lower in str(value).lower():
                results.append(row)
                break
        if len(results) >= limit:
            break
    return results
{% endif %}



{% if 'read' in ops %}
@mcp.tool()
def count_{{ table.name }}() -> int:
    """Get the total number of rows in '{{ table.description }}'."""
    ws = spreadsheet.worksheet(SHEET_NAMES["{{ table.name }}"])
    return len(ws.get_all_records())
{% endif %}



{% if 'read' in ops %}
@mcp.tool()
def schema_{{ table.name }}() -> dict:
    """Get the schema of the '{{ table.description }}' sheet."""
    return {
        {% for col in table.columns %}
        "{{ col.name }}": {"type": "{{ col.type.value }}", "sheet_header": "{{ col.description }}"},
        {% endfor %}
    }
{% endif %}


{% if 'insert' in ops or 'update' in ops or 'delete' in ops %}
@mcp.tool()
def append_{{ table.name }}({% for col in table.columns %}{{ col.name }}: str | None = None{{ ', ' if not loop.last else '' }}{% endfor %}) -> dict:
    """Append a new row to the '{{ table.description }}' sheet.

    Args:
        {% for col in table.columns %}
        {{ col.name }}: Value for '{{ col.description }}'.
        {% endfor %}

    Returns:
        The appended row data.
    """
    ws = spreadsheet.worksheet(SHEET_NAMES["{{ table.name }}"])
    headers = ws.row_values(1)
    row_values = []
    data = {
        {% for col in table.columns %}
        "{{ col.description }}": {{ col.name }},
        {% endfor %}
    }
    for h in headers:
        row_values.append(str(data.get(h, "") or ""))
    ws.append_row(row_values)
    return {"appended": True, "data": data}



{% if 'update' in ops %}
@mcp.tool()
def update_{{ table.name }}_cell(row_number: int, field: str, value: str) -> dict:
    """Update a single cell in '{{ table.description }}'.

    Args:
        row_number: The row number (from list results, 1-indexed).
        field: The safe field name to update.
        value: The new value.
    """
    ws = spreadsheet.worksheet(SHEET_NAMES["{{ table.name }}"])
    if field not in _{{ table.name }}_fields:
        return {"error": f"Unknown field: {field}"}
    original_name = _{{ table.name }}_fields[field]
    headers = ws.row_values(1)
    try:
        # Throttle API requests to prevent 429 Too Many Requests
        _rate_limiter.acquire()
        col_index = headers.index(original_name) + 1
    except ValueError:
        return {"error": f"Header '{original_name}' not found"}
    ws.update_cell(row_number, col_index, value)
    return {"updated": True, "row": row_number, "field": field, "value": value}
{% endif %}

{% endif %}
{% endfor %}

{% elif source_type == "notion" %}
# ─── Notion Connection ───

import os
from notion_client import Client as NotionClient

NOTION_API_KEY = os.environ.get("NOTION_API_KEY", os.environ.get("NOTION_TOKEN", ""))

if not NOTION_API_KEY:
    raise RuntimeError(
        "NOTION_API_KEY or NOTION_TOKEN environment variable is required. "
        "Create an integration at: https://www.notion.so/my-integrations"
    )

notion = NotionClient(auth=NOTION_API_KEY)

# Database ID mapping (safe name -> database ID)
DATABASE_IDS = {
    {% for table in tables %}
    "{{ table.name }}": "{{ schema.metadata.get('database_map', {}).get(table.name, '') }}",
    {% endfor %}
}

# Property name mapping per database
{% for table in tables %}
_{{ table.name }}_props = {
    {% for col in table.columns %}
    "{{ col.name }}": "{{ col.description }}",
    {% endfor %}
}
_{{ table.name }}_props_rev = {v: k for k, v in _{{ table.name }}_props.items()}
{% endfor %}


def _extract_notion_value(prop: dict):
    """Extract a plain Python value from a Notion property object."""
    prop_type = prop.get("type", "")

    if prop_type == "title":
        return "".join(p.get("plain_text", "") for p in prop.get("title", []))
    elif prop_type == "rich_text":
        return "".join(p.get("plain_text", "") for p in prop.get("rich_text", []))
    elif prop_type == "number":
        return prop.get("number")
    elif prop_type == "checkbox":
        return prop.get("checkbox", False)
    elif prop_type == "select":
        sel = prop.get("select")
        return sel.get("name", "") if sel else None
    elif prop_type == "multi_select":
        return [s.get("name", "") for s in prop.get("multi_select", [])]
    elif prop_type == "status":
        st = prop.get("status")
        return st.get("name", "") if st else None
    elif prop_type == "date":
        d = prop.get("date")
        return d.get("start") if d else None
    elif prop_type in ("url", "email", "phone_number"):
        return prop.get(prop_type)
    elif prop_type in ("created_time", "last_edited_time"):
        return prop.get(prop_type)
    elif prop_type == "people":
        return [p.get("name", p.get("id", "")) for p in prop.get("people", [])]
    elif prop_type == "files":
        return [f.get("name", "") for f in prop.get("files", [])]
    elif prop_type == "relation":
        return [r.get("id", "") for r in prop.get("relation", [])]
    elif prop_type == "formula":
        f = prop.get("formula", {})
        return f.get(f.get("type", ""))
    elif prop_type == "rollup":
        r = prop.get("rollup", {})
        rt = r.get("type", "")
        return r.get("array", []) if rt == "array" else r.get(rt)
    elif prop_type == "unique_id":
        uid = prop.get("unique_id", {})
        prefix = uid.get("prefix", "")
        num = uid.get("number", 0)
        return f"{prefix}-{num}" if prefix else str(num)
    elif prop_type in ("created_by", "last_edited_by"):
        user = prop.get(prop_type, {})
        return user.get("name", user.get("id", ""))
    return str(prop.get(prop_type, ""))


{% for table in tables %}

# ═══════════════════════════════════════════════════════
# Database: {{ table.description }} (as {{ table.name }})
# Properties: {{ table.columns | map(attribute='name') | join(', ') }}
{% if table.row_count is not none %}
# Pages: {{ table.row_count }}
{% endif %}
# ═══════════════════════════════════════════════════════


def _normalize_{{ table.name }}(page: dict) -> dict:
    """Convert a Notion page to a flat dict with safe property names."""
    result = {"page_id": page.get("id", "")}
    props = page.get("properties", {})
    for safe_name, original_name in _{{ table.name }}_props.items():
        if original_name in props:
            result[safe_name] = _extract_notion_value(props[original_name])
        else:
            result[safe_name] = None
    return result



{% if 'read' in ops %}
@mcp.tool()
def list_{{ table.name }}(limit: int = 50, start_cursor: str | None = None) -> dict:
    """List pages from the '{{ table.description }}' Notion database.

    Args:
        limit: Maximum pages (default: 50, max: 100).
        start_cursor: Pagination cursor from previous results.

    Returns:
        Dict with 'results' (list of pages) and 'next_cursor' for pagination.
    """
    limit = min(limit, 100)
    kwargs = {"database_id": DATABASE_IDS["{{ table.name }}"], "page_size": limit}
    if start_cursor:
        kwargs["start_cursor"] = start_cursor
    response = notion.databases.query(**kwargs)
    results = [_normalize_{{ table.name }}(p) for p in response.get("results", [])]
    return {
        "results": results,
        "next_cursor": response.get("next_cursor"),
        "has_more": response.get("has_more", False),
    }
{% endif %}



{% if 'read' in ops %}
@mcp.tool()
def get_{{ table.name }}_by_page_id(page_id: str) -> dict | None:
    """Get a single page from '{{ table.description }}' by its Notion page ID.

    Args:
        page_id: The Notion page ID.
    """
    try:
        # Throttle API requests to prevent 429 Too Many Requests
        _rate_limiter.acquire()
        page = notion.pages.retrieve(page_id=page_id)
        return _normalize_{{ table.name }}(page)
    except Exception:
        return None
{% endif %}



{% if 'read' in ops %}
@mcp.tool()
def search_{{ table.name }}(query: str, limit: int = 20) -> list[dict]:
    """Search '{{ table.description }}' for matching pages.

    Args:
        query: Search text (case-insensitive, matches visible text fields).
        limit: Maximum results.
    """
    # Notion's database query doesn't have a direct text search,
    # so we fetch all and filter client-side
    all_pages = []
    has_more = True
    cursor = None
    while has_more and len(all_pages) < 1000:
        result = list_{{ table.name }}(limit=100, start_cursor=cursor)
        all_pages.extend(result["results"])
        has_more = result["has_more"]
        cursor = result.get("next_cursor")

    query_lower = query.lower()
    results = []
    for page in all_pages:
        for value in page.values():
            if value and query_lower in str(value).lower():
                results.append(page)
                break
        if len(results) >= limit:
            break
    return results
{% endif %}


@mcp.tool()
def filter_{{ table.name }}(property_name: str, value: str) -> list[dict]:
    """Filter '{{ table.description }}' by a property value.

    Uses Notion's native filter API for efficient server-side filtering.

    Args:
        property_name: The safe property name to filter by.
        value: The value to match.

    Available properties:
        {% for col in table.columns %}
        {{ col.name }} ({{ col.type.value }}) -> {{ col.description }}
        {% endfor %}
    """
    if property_name not in _{{ table.name }}_props:
        return []
    original_name = _{{ table.name }}_props[property_name]

    # Build a simple equals filter (works for text, select, etc.)
    filter_obj = {
        "property": original_name,
        "rich_text": {"equals": value},
    }

    try:
        # Throttle API requests to prevent 429 Too Many Requests
        _rate_limiter.acquire()
        response = notion.databases.query(
            database_id=DATABASE_IDS["{{ table.name }}"],
            filter=filter_obj,
            page_size=100,
        )
        return [_normalize_{{ table.name }}(p) for p in response.get("results", [])]
    except Exception:
        # Fall back to client-side filter if filter type doesn't match
        all_pages = search_{{ table.name }}(value, limit=100)
        return [
            p for p in all_pages
            if p.get(property_name) and value.lower() in str(p[property_name]).lower()
        ]



{% if 'read' in ops %}
@mcp.tool()
def count_{{ table.name }}() -> int:
    """Get the total number of pages in '{{ table.description }}'."""
    count = 0
    has_more = True
    cursor = None
    while has_more:
        result = list_{{ table.name }}(limit=100, start_cursor=cursor)
        count += len(result["results"])
        has_more = result["has_more"]
        cursor = result.get("next_cursor")
    return count
{% endif %}



{% if 'read' in ops %}
@mcp.tool()
def schema_{{ table.name }}() -> dict:
    """Get the schema of '{{ table.description }}' Notion database.

    Returns property names, types, and available options for select fields.
    """
    {% set sel_opts = schema.metadata.get('select_options_map', {}).get(table.name, {}) %}
    return {
        {% for col in table.columns %}
        "{{ col.name }}": {
            "type": "{{ col.type.value }}",
            "notion_name": "{{ col.description }}",
            {% if col.name in sel_opts %}
            "options": {{ sel_opts[col.name] }},
            {% endif %}
        },
        {% endfor %}
    }
{% endif %}


{% if 'insert' in ops or 'update' in ops or 'delete' in ops %}
@mcp.tool()
def create_{{ table.name }}({% for col in table.columns %}{{ col.name }}: str | None = None{{ ', ' if not loop.last else '' }}{% endfor %}) -> dict:
    """Create a new page in '{{ table.description }}'.

    Args:
        {% for col in table.columns %}
        {{ col.name }}: Value for '{{ col.description }}'.
        {% endfor %}

    Returns:
        The created page with its page_id.
    """
    properties = {}
    {% for col in table.columns %}
    if {{ col.name }} is not None:
        properties["{{ col.description }}"] = {"rich_text": [{"text": {"content": str({{ col.name }})}}]}
    {% endfor %}
    page = notion.pages.create(
        parent={"database_id": DATABASE_IDS["{{ table.name }}"]},
        properties=properties,
    )
    return _normalize_{{ table.name }}(page)

{% endif %}
{% endfor %}

{% elif source_type == "files" %}
# ─── File Data Source ───

DATA_DIR = "{{ source_uri }}"

{% for table in tables %}

# ═══════════════════════════════════════════════════════
# File: {{ table.name }}
# Columns: {{ table.columns | map(attribute='name') | join(', ') }}
{% if table.row_count is not none %}
# Rows: {{ table.row_count }}
{% endif %}
# ═══════════════════════════════════════════════════════


{% if 'read' in ops %}
@mcp.tool()
def list_{{ table.name }}(limit: int = 50, offset: int = 0) -> list[dict]:
    """List records from the '{{ table.name }}' dataset.

    Args:
        limit: Maximum rows to return (default: 50, max: 500).
        offset: Number of rows to skip for pagination.

    Returns:
        List of records as dictionaries.
    """
    limit = min(limit, 500)
    filepath = os.path.join(DATA_DIR, "{{ table.name }}.csv")
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            rows = list(reader)
        return rows[offset:offset + limit]

    filepath = os.path.join(DATA_DIR, "{{ table.name }}.json")
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, list):
            return data[offset:offset + limit]
        return [data]

    return []
{% endif %}



{% if 'read' in ops %}
@mcp.tool()
def search_{{ table.name }}(query: str, limit: int = 20) -> list[dict]:
    """Search the '{{ table.name }}' dataset for matching records.

    Args:
        query: Text to search for (case-insensitive, matches any field).
        limit: Maximum results to return.

    Returns:
        List of matching records.
    """
    all_rows = list_{{ table.name }}(limit=10000)
    query_lower = query.lower()
    results = []
    for row in all_rows:
        for value in row.values():
            if query_lower in str(value).lower():
                results.append(row)
                break
        if len(results) >= limit:
            break
    return results
{% endif %}



{% if 'read' in ops %}
@mcp.tool()
def count_{{ table.name }}() -> int:
    """Get the total number of records in the '{{ table.name }}' dataset."""
    return len(list_{{ table.name }}(limit=100000))
{% endif %}

{% endfor %}

{% for resource in resources %}
@mcp.resource("{{ resource.name }}://content")
def read_{{ resource.name | replace('-', '_') | replace(' ', '_') }}() -> str:
    """Read the contents of the '{{ resource.name }}' file.

    Returns:
        The full text content of the file.
    """
    with open("{{ resource.uri }}", "r", encoding="utf-8") as f:
        return f.read()

{% endfor %}
{% endif %}

{% if semantic %}
# ─── Semantic Search (ChromaDB Vector Search) ───

import chromadb
from chromadb.config import Settings as ChromaSettings

_chroma_client = chromadb.PersistentClient(
    path=".chroma",
    settings=ChromaSettings(anonymized_telemetry=False),
)

def _get_text_for_record(record: dict) -> str:
    """Combine all text fields of a record into a single searchable string."""
    parts = []
    for key, value in record.items():
        if value is not None and isinstance(value, str) and value.strip():
            parts.append(f"{key}: {value}")
        elif value is not None and not isinstance(value, (bytes, bytearray)):
            parts.append(f"{key}: {value}")
    return " | ".join(parts)

{% for table in tables %}
# ─── Semantic Index: {{ table.name }} ───

def _build_index_{{ table.name }}():
    """Build or update the semantic index for '{{ table.name }}'."""
    collection = _chroma_client.get_or_create_collection(
        name="{{ table.name }}",
        metadata={"hnsw:space": "cosine"},
    )
    # Skip if already populated
    if collection.count() > 0:
        return collection

    # Fetch all records
    {% if source_type == "sqlite" %}
    conn = sqlite3.connect("{{ source_uri.replace('sqlite:///', '') }}")
    conn.row_factory = sqlite3.Row
    cursor = conn.execute("SELECT * FROM {{ table.name }}")
    rows = [dict(row) for row in cursor.fetchall()]
    conn.close()
    {% elif source_type == "files" %}
    rows = list_{{ table.name }}(limit=100000)
    {% elif source_type in ("postgres", "postgresql") %}
    conn = psycopg2.connect("{{ source_uri }}")
    cursor = conn.cursor(cursor_factory=RealDictCursor)
    cursor.execute("SELECT * FROM {{ table.name }}")
    rows = cursor.fetchall()
    conn.close()
    {% elif source_type == "mysql" %}
    conn = pymysql.connect(
        host=_parsed.hostname, port=_parsed.port or 3306,
        user=_parsed.username, password=_parsed.password,
        database=_parsed.path.lstrip("/"),
        cursorclass=pymysql.cursors.DictCursor,
    )
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM {{ table.name }}")
    rows = cursor.fetchall()
    conn.close()
    {% elif source_type == "airtable" %}
    rows = list_{{ table.name }}(limit=100000)
    {% elif source_type == "gsheet" %}
    rows = list_{{ table.name }}(limit=100000)
    {% elif source_type == "notion" %}
    rows = list_{{ table.name }}(limit=100000).get("results", [])
    {% endif %}

    if not rows:
        return collection

    # Embed in batches of 100
    batch_size = 100
    for i in range(0, len(rows), batch_size):
        batch = rows[i:i + batch_size]
        documents = [_get_text_for_record(row) for row in batch]
        ids = [f"{{ table.name }}_{i + j}" for j in range(len(batch))]
        metadatas = []
        for row in batch:
            meta = {}
            for k, v in row.items():
                if v is not None and not isinstance(v, (bytes, bytearray, list, dict)):
                    meta[str(k)] = str(v)[:500]
            metadatas.append(meta)
        collection.add(documents=documents, ids=ids, metadatas=metadatas)

    return collection


@mcp.tool()
def semantic_search_{{ table.name }}(query: str, limit: int = 10) -> list[dict]:
    """Search '{{ table.name }}' by meaning using vector similarity.

    Unlike keyword search, this finds results based on semantic meaning.
    For example, searching 'artificial intelligence' will also find
    records about 'machine learning', 'deep learning', etc.

    Args:
        query: Natural language search query.
        limit: Maximum number of results (default: 10).

    Returns:
        A list of matching records ranked by relevance, each with a
        'similarity_score' field (0-1, higher = more relevant).
    """
    collection = _build_index_{{ table.name }}()
    if collection.count() == 0:
        return []

    results = collection.query(
        query_texts=[query],
        n_results=min(limit, collection.count()),
    )

    output = []
    for i in range(len(results["ids"][0])):
        record = dict(results["metadatas"][0][i]) if results["metadatas"][0][i] else {}
        distance = results["distances"][0][i] if results["distances"] else None
        # Convert cosine distance to similarity score (0-1)
        record["similarity_score"] = round(1 - distance, 4) if distance is not None else None
        record["_document"] = results["documents"][0][i][:200] if results["documents"] else ""
        output.append(record)

    return output


@mcp.tool()
def rebuild_index_{{ table.name }}() -> dict:
    """Force rebuild the semantic search index for '{{ table.name }}'.

    Use this if the underlying data has changed and the search index
    is out of date. This will delete and recreate the vector index.

    Returns:
        Status of the rebuild operation.
    """
    try:
        # Throttle API requests to prevent 429 Too Many Requests
        _rate_limiter.acquire()
        _chroma_client.delete_collection("{{ table.name }}")
    except Exception:
        pass
    collection = _build_index_{{ table.name }}()
    return {
        "status": "rebuilt",
        "table": "{{ table.name }}",
        "indexed_records": collection.count(),
    }

{% endfor %}
{% endif %}

# ─── Run Server ───

if __name__ == "__main__":
    mcp.run()

