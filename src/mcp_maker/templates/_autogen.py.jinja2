"""
Auto-generated MCP Server tools by MCP-Maker
Source: {{ source_type }}

⚠️ DO NOT EDIT THIS FILE DIRECTLY ⚠️
This file is overwritten every time you run `mcp-maker init`.
Add your custom tools to `{{ target_filename }}` instead.
"""

import traceback
import os
import sys

# Load connection string from environment to prevent credential exposure in source code
DATABASE_URL = os.environ.get("DATABASE_URL")

{% if audit %}
import json
import logging
from datetime import datetime

# Configure audit logging
logger = logging.getLogger("mcp-maker.audit")
logger.setLevel(logging.INFO)
if not logger.handlers:
    fh = logging.FileHandler("mcp_audit.log")
    fh.setFormatter(logging.Formatter('%(message)s'))
    logger.addHandler(fh)

def log_audit_event(tool_name: str, args: dict, status: str, error: str = None):
    event = {
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "tool": tool_name,
        "arguments": args,
        "status": status,
    }
    if error:
        event["error"] = error
    logger.info(json.dumps(event))

{% endif %}

{% if source_type == "sqlite" %}
import sqlite3
import threading
{% elif source_type == "postgres" %}
import psycopg2
import psycopg2.extras
import psycopg2.pool
{% elif source_type == "mysql" %}
import pymysql
import pymysql.cursors
{% elif source_type == "files" %}
import csv
import json
import os as _os
{% endif %}

{% if source_type in ["airtable", "notion", "gsheets"] %}
import time
import threading

class TokenBucketRateLimiter:
    """A thread-safe token bucket rate limiter to prevent API 429s from parallel LLM calls."""
    def __init__(self, target_rps: float, burst_size: int = 1):
        self._target_rps = target_rps
        self._capacity = burst_size
        self._tokens = burst_size
        self._last_refill = time.monotonic()
        self._lock = threading.Lock()

    def acquire(self):
        with self._lock:
            while True:
                now = time.monotonic()
                elapsed = now - self._last_refill
                if elapsed > 0:
                    self._tokens = min(self._capacity, self._tokens + elapsed * self._target_rps)
                    self._last_refill = now
                
                if self._tokens >= 1.0:
                    self._tokens -= 1.0
                    return
                time.sleep(0.1)

# Default to 4 requests per second to stay safely under 5 req/s limits
_rate_limiter = TokenBucketRateLimiter(target_rps=4.0, burst_size=1)
{% endif %}

from mcp.server.fastmcp import FastMCP

# ─── Server Setup ───

mcp = FastMCP("{{ source_type }}-server")

{% if audit %}
_original_tool = mcp.tool

def _audit_tool_decorator(*args, **kwargs):
    def decorator(func):
        import functools
        @functools.wraps(func)
        def wrapper(*w_args, **w_kwargs):
            try:
                result = func(*w_args, **w_kwargs)
                log_audit_event(func.__name__, w_kwargs, "success")
                return result
            except Exception as e:
                log_audit_event(func.__name__, w_kwargs, "error", str(e))
                raise
        return _original_tool(*args, **kwargs)(wrapper)
    return decorator

mcp.tool = _audit_tool_decorator
{% endif %}

{% if source_type == "sqlite" %}
{% include "partials/sqlite.jinja2" %}
{% elif source_type == "postgres" %}
{% include "partials/postgres.jinja2" %}
{% elif source_type == "mysql" %}
{% include "partials/mysql.jinja2" %}
{% elif source_type == "airtable" %}
{% include "partials/airtable.jinja2" %}
{% elif source_type == "gsheet" %}
{% include "partials/gsheets.jinja2" %}
{% elif source_type == "notion" %}
{% include "partials/notion.jinja2" %}
{% elif source_type == "files" %}
{% include "partials/files.jinja2" %}
{% endif %}

{% if semantic %}
{% include "partials/semantic_search.jinja2" %}
{% endif %}
