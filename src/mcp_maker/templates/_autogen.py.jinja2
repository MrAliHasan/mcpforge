"""
Auto-generated MCP Server tools by MCP-Maker
Source: {{ source_type }}

⚠️ DO NOT EDIT THIS FILE DIRECTLY ⚠️
This file is overwritten every time you run `mcp-maker init`.
Add your custom tools to `{{ target_filename }}` instead.
"""

import os

# Load connection string from environment to prevent credential exposure in source code
DATABASE_URL = os.environ.get("DATABASE_URL")

{% if audit %}
import json
import logging
from datetime import datetime, timezone

# Configure audit logging
logger = logging.getLogger("mcp-maker.audit")
logger.setLevel(logging.INFO)
if not logger.handlers:
    fh = logging.FileHandler("mcp_audit.log")
    fh.setFormatter(logging.Formatter('%(message)s'))
    logger.addHandler(fh)

def log_audit_event(tool_name: str, args: dict, status: str, error: str = None):
    event = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "tool": tool_name,
        "arguments": args,
        "status": status,
    }
    if error:
        event["error"] = error
    logger.info(json.dumps(event))

{% endif %}

{% if source_type == "sqlite" %}
{% if async_mode %}
import aiosqlite
{% else %}
import sqlite3
import threading
{% endif %}
{% elif source_type == "postgres" %}
{% if async_mode %}
import asyncpg
{% else %}
import psycopg2
import psycopg2.extras
import psycopg2.pool
{% endif %}
{% elif source_type == "mysql" %}
{% if async_mode %}
import aiomysql
{% else %}
import pymysql
import pymysql.cursors
{% endif %}
{% elif source_type == "files" %}
import csv
import json
import os as _os
{% elif source_type == "excel" %}
import openpyxl
{% elif source_type == "mongodb" %}
from pymongo import MongoClient
from bson import ObjectId
from urllib.parse import urlparse
{% elif source_type == "supabase" %}
from supabase import create_client
{% elif source_type == "openapi" %}
import httpx
{% elif source_type == "redis" %}
import redis as _redis_lib
from urllib.parse import urlparse as _urlparse
{% endif %}

{% if rate_limit > 0 or source_type in ["airtable", "notion", "gsheets"] %}
import time
import threading

class TokenBucketRateLimiter:
    """A thread-safe token bucket rate limiter to prevent API 429s from parallel LLM calls."""
    def __init__(self, target_rps: float, burst_size: int = 1):
        self._target_rps = target_rps
        self._capacity = burst_size
        self._tokens = burst_size
        self._last_refill = time.monotonic()
        self._lock = threading.Lock()

    def acquire(self):
        while True:
            with self._lock:
                now = time.monotonic()
                elapsed = now - self._last_refill
                if elapsed > 0:
                    self._tokens = min(self._capacity, self._tokens + elapsed * self._target_rps)
                    self._last_refill = now
                
                if self._tokens >= 1.0:
                    self._tokens -= 1.0
                    return
            # Sleep OUTSIDE the lock so other threads aren't blocked
            time.sleep(0.1)

{% if source_type in ["airtable", "notion", "gsheets"] %}
# Default to 4 requests per second to stay safely under 5 req/s limits
_rate_limiter = TokenBucketRateLimiter(target_rps=4.0, burst_size=1)
{% endif %}
{% endif %}

from mcp.server.fastmcp import FastMCP

# ─── Server Setup ───

mcp = FastMCP("{{ source_type }}-server")

{% if rate_limit > 0 %}
# ─── Global Rate Limiter ───

_global_rate_limiter = TokenBucketRateLimiter(target_rps={{ rate_limit }}, burst_size=max(1, int({{ rate_limit }} / 2)))
_original_tool_rl = mcp.tool

def _rate_limit_tool_decorator(*args, **kwargs):
    def decorator(func):
        import functools
        @functools.wraps(func)
        def wrapper(*w_args, **w_kwargs):
            _global_rate_limiter.acquire()
            return func(*w_args, **w_kwargs)
        return _original_tool_rl(*args, **kwargs)(wrapper)
    return decorator

mcp.tool = _rate_limit_tool_decorator
{% endif %}

{% if otel %}
# ─── OpenTelemetry Integration (Enterprise Audit) ───
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter

# Initialize tracer
trace.set_tracer_provider(TracerProvider())
trace.get_tracer_provider().add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))
_otel_tracer = trace.get_tracer("{{ source_type }}-server")

_original_tool_otel = mcp.tool

def _otel_tool_decorator(*args, **kwargs):
    def decorator(func):
        import functools
        import inspect

        @functools.wraps(func)
        def wrapper(*w_args, **w_kwargs):
            with _otel_tracer.start_as_current_span(func.__name__) as span:
                span.set_attribute("mcp.tool.args", str(w_kwargs))
                try:
                    return func(*w_args, **w_kwargs)
                except Exception as e:
                    span.record_exception(e)
                    span.set_status(trace.Status(trace.StatusCode.ERROR))
                    raise

        @functools.wraps(func)
        async def async_wrapper(*w_args, **w_kwargs):
            with _otel_tracer.start_as_current_span(func.__name__) as span:
                span.set_attribute("mcp.tool.args", str(w_kwargs))
                try:
                    return await func(*w_args, **w_kwargs)
                except Exception as e:
                    span.record_exception(e)
                    span.set_status(trace.Status(trace.StatusCode.ERROR))
                    raise

        if inspect.iscoroutinefunction(func):
            return _original_tool_otel(*args, **kwargs)(async_wrapper)
        return _original_tool_otel(*args, **kwargs)(wrapper)
    return decorator

mcp.tool = _otel_tool_decorator
{% endif %}

{% if auth_mode == "api-key" %}
# ─── API Key Authentication ───

_MCP_API_KEY = os.environ.get("MCP_API_KEY")
if not _MCP_API_KEY:
    import warnings
    warnings.warn(
        "MCP_API_KEY is not set. All requests will be rejected. "
        "Set MCP_API_KEY environment variable to enable access.",
        stacklevel=1,
    )

_original_tool_auth = mcp.tool

def _auth_tool_decorator(*args, **kwargs):
    def decorator(func):
        import functools
        @functools.wraps(func)
        def wrapper(*w_args, **w_kwargs):
            # Check for API key in environment (set by transport layer)
            if not _MCP_API_KEY:
                raise PermissionError("Server API key not configured. Set MCP_API_KEY env var.")
            return func(*w_args, **w_kwargs)
        return _original_tool_auth(*args, **kwargs)(wrapper)
    return decorator

mcp.tool = _auth_tool_decorator
{% endif %}

{% if audit %}
_original_tool = mcp.tool

def _audit_tool_decorator(*args, **kwargs):
    def decorator(func):
        import functools
        @functools.wraps(func)
        def wrapper(*w_args, **w_kwargs):
            try:
                result = func(*w_args, **w_kwargs)
                log_audit_event(func.__name__, w_kwargs, "success")
                return result
            except Exception as e:
                log_audit_event(func.__name__, w_kwargs, "error", str(e))
                raise
        return _original_tool(*args, **kwargs)(wrapper)
    return decorator

mcp.tool = _audit_tool_decorator
{% endif %}

{% if cache_ttl > 0 %}
{% if cache_backend.startswith('redis://') %}
# ─── Redis Smart Cache (TTL={{ cache_ttl }}s) ───

import redis as _cache_redis
import json as _cache_json

_redis_client = _cache_redis.from_url("{{ cache_backend }}", decode_responses=True)

class _RedisCache:
    def __init__(self, ttl: int = {{ cache_ttl }}):
        self._ttl = ttl
        self._prefix = "mcp_cache:{{ source_type }}:"
        self._hits = 0
        self._misses = 0

    def get(self, key: str):
        full_key = self._prefix + key
        try:
            val = _redis_client.get(full_key)
            if val:
                self._hits += 1
                return _cache_json.loads(val)
        except Exception:
            pass
        self._misses += 1
        return None

    def set(self, key: str, value):
        full_key = self._prefix + key
        try:
            _redis_client.setex(full_key, self._ttl, _cache_json.dumps(value, default=str))
        except Exception:
            pass

    def clear(self):
        try:
            keys = _redis_client.keys(self._prefix + "*")
            if keys:
                _redis_client.delete(*keys)
            return len(keys)
        except Exception:
            return 0

    def stats(self) -> dict:
        try:
            info = _redis_client.info()
            return {
                "backend": "redis",
                "connected": True,
                "ttl_seconds": self._ttl,
                "hits": self._hits,
                "misses": self._misses,
                "hit_rate": f"{self._hits / max(1, self._hits + self._misses) * 100:.1f}%",
                "memory_used_human": info.get("used_memory_human", "unknown"),
            }
        except Exception as e:
            return {"backend": "redis", "connected": False, "error": str(e)}

_cache = _RedisCache(ttl={{ cache_ttl }})

{% else %}
# ─── In-Memory Smart Cache (TTL={{ cache_ttl }}s) ───

import time as _cache_time
import threading as _cache_threading

class _SmartCache:
    """Thread-safe TTL-based in-memory cache with max-size eviction for read tools."""

    def __init__(self, ttl: int = {{ cache_ttl }}, max_entries: int = 1000):
        self._ttl = ttl
        self._max_entries = max_entries
        self._store: dict[str, tuple[float, object]] = {}
        self._lock = _cache_threading.Lock()
        self._hits = 0
        self._misses = 0

    def get(self, key: str):
        with self._lock:
            if key in self._store:
                ts, value = self._store[key]
                if _cache_time.time() - ts < self._ttl:
                    self._hits += 1
                    return value
                del self._store[key]
            self._misses += 1
            return None

    def set(self, key: str, value):
        with self._lock:
            # Evict oldest entries if at capacity
            if len(self._store) >= self._max_entries and key not in self._store:
                oldest_key = min(self._store, key=lambda k: self._store[k][0])
                del self._store[oldest_key]
            self._store[key] = (_cache_time.time(), value)

    def clear(self):
        with self._lock:
            count = len(self._store)
            self._store.clear()
            return count

    def stats(self) -> dict:
        with self._lock:
            return {
                "backend": "memory",
                "entries": len(self._store),
                "max_entries": self._max_entries,
                "hits": self._hits,
                "misses": self._misses,
                "hit_rate": f"{self._hits / max(1, self._hits + self._misses) * 100:.1f}%",
                "ttl_seconds": self._ttl,
            }

_cache = _SmartCache(ttl={{ cache_ttl }})
{% endif %}

_original_tool_for_cache = mcp.tool

def _cached_tool_decorator(*args, **kwargs):
    """Decorator that caches read-only tool results for {{ cache_ttl }}s."""
    def decorator(func):
        import functools
        @functools.wraps(func)
        def wrapper(*w_args, **w_kwargs):
            # Only cache read-like tools (list_, search_, count_, get_, schema_)
            fn_name = func.__name__
            is_read = any(fn_name.startswith(p) for p in ("list_", "search_", "count_", "get_", "schema_"))
            if not is_read:
                return func(*w_args, **w_kwargs)

            cache_key = f"{fn_name}:{repr(w_args)}:{repr(sorted(w_kwargs.items()))}"
            cached = _cache.get(cache_key)
            if cached is not None:
                return cached
            result = func(*w_args, **w_kwargs)
            _cache.set(cache_key, result)
            return result
        return _original_tool_for_cache(*args, **kwargs)(wrapper)
    return decorator

mcp.tool = _cached_tool_decorator

@_original_tool_for_cache()
def cache_clear_all() -> dict:
    """Clear all cached tool results."""
    cleared = _cache.clear()
    return {"cleared": cleared, "status": "ok"}

@_original_tool_for_cache()
def cache_stats() -> dict:
    """Get cache hit/miss statistics."""
    return _cache.stats()

{% endif %}

{% if source_type == "sqlite" %}
{% if async_mode %}
{% include "partials/sqlite_async.jinja2" %}
{% else %}
{% include "partials/sqlite.jinja2" %}
{% endif %}
{% elif source_type == "postgres" %}
{% if async_mode %}
{% include "partials/postgres_async.jinja2" %}
{% else %}
{% include "partials/postgres.jinja2" %}
{% endif %}
{% elif source_type == "mysql" %}
{% if async_mode %}
{% include "partials/mysql_async.jinja2" %}
{% else %}
{% include "partials/mysql.jinja2" %}
{% endif %}
{% elif source_type == "airtable" %}
{% include "partials/airtable.jinja2" %}
{% elif source_type == "gsheet" %}
{% include "partials/gsheets.jinja2" %}
{% elif source_type == "notion" %}
{% include "partials/notion.jinja2" %}
{% elif source_type == "files" %}
{% include "partials/files.jinja2" %}
{% elif source_type == "excel" %}
{% include "partials/excel.jinja2" %}
{% elif source_type == "mongodb" %}
{% include "partials/mongodb.jinja2" %}
{% elif source_type == "supabase" %}
{% include "partials/supabase.jinja2" %}
{% elif source_type == "openapi" %}
{% include "partials/openapi.jinja2" %}
{% elif source_type == "redis" %}
{% include "partials/redis.jinja2" %}
{% endif %}

{% if semantic %}
{% include "partials/semantic_search.jinja2" %}
{% endif %}

{% include "partials/system_tools.jinja2" %}

{% include "partials/export_tools.jinja2" %}

{% include "partials/webhook_tools.jinja2" %}
